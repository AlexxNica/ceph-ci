
overrides:
  ceph:
    cephfs_ec_profile:
      - disabled
    # test_full makes assumptions about the number of bytes that need to be
    # written before the pool becomes full. In earlier versions of this QA
    # suite, we used 3 OSDs which simplified things because every object would
    # have a replica on every OSD. Now we have up to 6 OSDs to support EC pool
    # testing so now we need to deal with uneven distribution of objects across
    # OSDs so those assumptions break down. For simplicity, we now use 1 PG
    # which will bring us back to using 3 OSDs of the 3+ available.
    cephfs_metadata_pool:
      pgs: 64
    cephfs_data_pool:
      pgs: 64
    log-whitelist:
      - OSD full dropping all updates
      - OSD near full
      - failsafe engaged, dropping updates
      - failsafe disengaged, no longer dropping
      - is full \(reached quota
    conf:
      mon:
        mon osd nearfull ratio: 0.6
        mon osd backfillfull ratio: 0.6
        mon osd full ratio: 0.7
      osd:
        osd mon report interval max: 5
        osd objectstore: memstore
        osd failsafe full ratio: 1.0
        memstore device bytes: 200000000
      client.0:
        debug client: 20
        debug objecter: 20
        debug objectcacher: 20
      client.1:
        debug client: 20
        debug objecter: 20
        debug objectcacher: 20

tasks:
  - cephfs_test_runner:
      modules:
        - tasks.cephfs.test_full
